# Language 语言
*- 项目分枝：
  - Parser 解释器/编译器（项目主要解决‘语法结构的分析’，而不是‘语义’的判断）
  - Attention 注意力机制（？？？）*

## Attention
### 题干
编写一个人工智能来预测文本序列中的屏蔽词。
```zsh
$ python mask.py
Text: We turned down a narrow lane and passed through a small [MASK].
We turned down a narrow lane and passed through a small field.
We turned down a narrow lane and passed through a small clearing.
We turned down a narrow lane and passed through a small park.

$ python mask.py
Text: Then I picked up a [MASK] from the table.
Then I picked up a book from the table.
Then I picked up a bottle from the table.
Then I picked up a plate from the table.
```
#### 背景   
创建语言模型的一种方法是建立掩码/遮罩语言模型（Masked Language Model），即训练语言模型来预测文本序列中缺少的 “掩码/遮罩”单词。BERT是谷歌开发的一种基于**转换器transformer**的语言模型，它就是用这种方法训练出来的：根据周围的 *上下文词语* ，训练语言模型来预测一个屏蔽词/遮罩词。  
BERT 采用**转换器架构 transformer architecture**，因此使用**注意力机制 attention**来理解语言。在基础 BERT 模型中，转换器使用 12 层，每层有 12 个自我注意头，总共有 144 个自我注意头。  
本项目包括两个部分：  
首先，我们将使用人工智能软件公司 Hugging Face 开发的**Transformer Python library 转换器py库**，编写一个使用 BERT 预测屏蔽词的程序。该程序还将生成可视化注意力得分图表，144 个注意力头各生成一个图表。  
其次，我们将对程序生成的图表进行分析，试图 *（目标之一：）了解 BERT 的注意力头在试图理解我们的自然语言时可能会关注哪些内容*。  
#### 理解  
首先来看一下 mask.py 程序。在主函数（main function）中，首先会提示用户输入一些文本。输入文本应包含一个遮罩标记 [MASK]（a mask token[MASK]），代表我们的语言模型应尝试预测的单词。然后，本函数（function）会**使用自动标记器（AutoTokenizer）将输入内容分割成标记（tokens）**。  

在 BERT 模型中，每个不同的标记（token）都有自己的 ID 号。其中一个 ID 由 **tokenizer.mask_token_id** 提供，对应于 [MASK] 标记。大多数其他标记代表单词，但也有一些例外。[CLS] 标记总是出现在文本序列的开头。[SEP]标记出现在文本序列的末尾，用于分隔序列。有时，一个单词会被拆分成多个标记：例如，BERT 将单词 “intelligently ”视为两个标记：intelligent 和 ##ly。  

接下来，我们使用 **[`TFBertForMaskedLM`](https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/bert#transformers.TFBertForMaskedLM)** 做实例，使用 BERT 语言模型预测一个被屏蔽的标记。输入标记（inputs）被传入模型，然后我们寻找前 K 个输出标记（output tokens）。原始序列被打印出来，屏蔽标记被每个预测输出标记替换（参考上面的zsh部份代码举例）。  

最后，程序调用 visualize_attentions 函数，该函数将为 BERT 的每个注意力头（attention heads）生成输入序列的注意力值（attention values）图。  

大部分代码已经为您编写完成，但 get_mask_token_index、get_color_for_attention_score 和 visualize_attentions 的实现则需要您自己完成！  

完成这三个函数后，mask.py 程序就会生成注意力图。通过这些图，我们可以了解 BERT 在理解语言时需要注意的事项。例如，下面是第 3 层第 10 个头在处理句子 “然后我从桌子上拿起了一个 [MASK]”时的注意图。  

![image](https://github.com/HanhanXing/-2024/assets/49121375/abcdc534-6ae0-433e-a6b8-a77ffcaabafb)  

回想一下，浅色代表较高的注意力权重（attention weight），深色代表较低的注意力权重。在这种情况下，这个注意头似乎已经学会了一个非常清晰的模式：每个单词都在注意紧随其后的单词。例如，“then ”这个词在图中的第二行中，最亮的单元格是与 “i ”列相对应的单元格，这表明 “then ”这个词对 “i ”这个词的注意力很强（attending strongly to）。句子中的其他标记（tokens）也是如此。  

您可以尝试在其他句子中运行 mask.py，以验证第 3 层第 10 个头是否继续遵循这种模式。从直觉上讲（intuitively），BERT 有可能学会识别这种模式：理解一连串文本中的某个单词往往取决于知道下一个单词是什么，因此有一个（或多个）注意力头专门关注下一个单词是什么可能很有用。   

这个注意头特别清晰，但通常注意头会比较嘈杂（noisier），可能需要更多的解释才能猜出 BERT 可能在注意什么。  

比如说，我们很想知道 BERT 是否关注副词的作用。我们可以给这个模型一个句子，比如 “乌龟慢慢地穿过[MASK]”，然后观察所得到的注意头，看看语言模型是否注意到 “慢慢地 ”是修饰 “moved ”的副词。在得到的注意力图中，第 4 层第 11 个头可能会引起您的注意。  

![image](https://github.com/HanhanXing/-2024/assets/49121375/5cef996d-a34a-45d1-a28e-d24019b68d02)  

这个注意头肯定比较嘈杂：这个注意头到底在做什么并不是一眼就能看出来的。但是请注意，对于副词 “慢慢地”，它最关注的是它所修饰的动词： “移动”。如果我们调换动词和副词的顺序，情况也是一样。
![image](https://github.com/HanhanXing/-2024/assets/49121375/8147c257-527e-4a8c-b100-b3107cf6b332)  

甚至在副词和它所修饰的动词没有直接挨在一起的句子中也是如此。  
![image](https://github.com/HanhanXing/-2024/assets/49121375/24b87218-1fea-40b5-a560-991b6b4c109f)  

因此，我们有理由猜测（reasonably guess），这个注意头已经学会了注意副词和它们修饰的词之间的关系。注意层并不总是与我们对单词间特定关系的预期一致，**也并不总是与人类可解释的（human-interpretable）关系完全一致**，但我们可以根据它们看起来的对应关系进行猜测--至少在这个项目中，你得这么做！



### 代码和代码结构
### 分段解读
### 最终成品
### 其他问题/经验备注

## Parser
### 题干
### 代码和代码结构
### 分段解读
### 最终成品
### 其他问题/经验备注
