# Language 语言
*- 项目分枝：
  - Parser 解释器/编译器（项目主要解决‘语法结构的分析’，而不是‘语义’的判断）
  - Attention 注意力机制（？？？）*

## Attention
### 题干
编写一个人工智能来预测文本序列中的屏蔽词。
```zsh
$ python mask.py
Text: We turned down a narrow lane and passed through a small [MASK].
We turned down a narrow lane and passed through a small field.
We turned down a narrow lane and passed through a small clearing.
We turned down a narrow lane and passed through a small park.

$ python mask.py
Text: Then I picked up a [MASK] from the table.
Then I picked up a book from the table.
Then I picked up a bottle from the table.
Then I picked up a plate from the table.
```
#### 背景   
创建语言模型的一种方法是建立掩码/遮罩语言模型（Masked Language Model），即训练语言模型来预测文本序列中缺少的 “掩码/遮罩”单词。BERT是谷歌开发的一种基于**转换器transformer**的语言模型，它就是用这种方法训练出来的：根据周围的 *上下文词语* ，训练语言模型来预测一个屏蔽词/遮罩词。  
BERT 采用**转换器架构 transformer architecture**，因此使用**注意力机制 attention**来理解语言。在基础 BERT 模型中，转换器使用 12 层，每层有 12 个自我注意头，总共有 144 个自我注意头。  
本项目包括两个部分：  
首先，我们将使用人工智能软件公司 Hugging Face 开发的**Transformer Python library 转换器py库**，编写一个使用 BERT 预测屏蔽词的程序。该程序还将生成可视化注意力得分图表，144 个注意力头各生成一个图表。  
其次，我们将对程序生成的图表进行分析，试图 *（目标之一：）了解 BERT 的注意力头在试图理解我们的自然语言时可能会关注哪些内容*。  
#### 理解  
首先来看一下 mask.py 程序。在主函数（main function）中，首先会提示用户输入一些文本。输入文本应包含一个遮罩标记 [MASK]（a mask token[MASK]），代表我们的语言模型应尝试预测的单词。然后，本函数（function）会**使用自动标记器（AutoTokenizer）将输入内容分割成标记（tokens）**。  

在 BERT 模型中，每个不同的标记（token）都有自己的 ID 号。其中一个 ID 由 **tokenizer.mask_token_id** 提供，对应于 [MASK] 标记。大多数其他标记代表单词，但也有一些例外。[CLS] 标记总是出现在文本序列的开头。[SEP]标记出现在文本序列的末尾，用于分隔序列。有时，一个单词会被拆分成多个标记：例如，BERT 将单词 “intelligently ”视为两个标记：intelligent 和 ##ly。  

接下来，我们使用 **[`TFBertForMaskedLM`](https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/bert#transformers.TFBertForMaskedLM)** 做实例，使用 BERT 语言模型预测一个被屏蔽的标记。输入标记（inputs）被传入模型，然后我们寻找前 K 个输出标记（output tokens）。原始序列被打印出来，屏蔽标记被每个预测输出标记替换（参考上面的zsh部份代码举例）。  

最后，程序调用 visualize_attentions 函数，该函数将为 BERT 的每个注意力头（attention heads）生成输入序列的注意力值（attention values）图。  

大部分代码已经为您编写完成，但 get_mask_token_index、get_color_for_attention_score 和 visualize_attentions 的实现则需要您自己完成！  

完成这三个函数后，mask.py 程序就会生成注意力图。通过这些图，我们可以了解 BERT 在理解语言时需要注意的事项。例如，下面是第 3 层第 10 个头在处理句子 “然后我从桌子上拿起了一个 [MASK]”时的注意图。  

![image](https://github.com/HanhanXing/-2024/assets/49121375/abcdc534-6ae0-433e-a6b8-a77ffcaabafb)



### 代码和代码结构
### 分段解读
### 最终成品
### 其他问题/经验备注

## Parser
### 题干
### 代码和代码结构
### 分段解读
### 最终成品
### 其他问题/经验备注
