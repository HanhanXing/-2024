# Language 语言
*- 项目分枝：
  - Parser 解释器/编译器（项目主要解决‘语法结构的分析’，而不是‘语义’的判断）
  - Attention 注意力机制（？？？）*

# Attention
## 题干
编写一个人工智能来预测文本序列中的屏蔽词。
```zsh
$ python mask.py
Text: We turned down a narrow lane and passed through a small [MASK].
We turned down a narrow lane and passed through a small field.
We turned down a narrow lane and passed through a small clearing.
We turned down a narrow lane and passed through a small park.

$ python mask.py
Text: Then I picked up a [MASK] from the table.
Then I picked up a book from the table.
Then I picked up a bottle from the table.
Then I picked up a plate from the table.
```
**背景**
创建语言模型的一种方法是建立掩码/遮罩语言模型（Masked Language Model），即训练语言模型来预测文本序列中缺少的 “掩码/遮罩”单词。BERT是谷歌开发的一种基于**转换器transformer**的语言模型，它就是用这种方法训练出来的：根据周围的 *上下文词语* ，训练语言模型来预测一个屏蔽词/遮罩词。

BERT 采用**转换器架构 transformer architecture**，因此使用**注意力机制 attention**来理解语言。在基础 BERT 模型中，转换器使用 12 层，每层有 12 个自我注意头，总共有 144 个自我注意头。

本项目包括两个部分：

首先，我们将使用人工智能软件公司 Hugging Face 开发的**Transformer Python library 转换器py库**，编写一个使用 BERT 预测屏蔽词的程序。该程序还将生成可视化注意力得分图表，144 个注意力头各生成一个图表。

其次，我们将对程序生成的图表进行分析，试图了解 BERT 的注意力头在试图理解我们的自然语言时可能会关注哪些内容。



